{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5090292c",
   "metadata": {},
   "source": [
    "# NetGuard-GNN\n",
    "\n",
    "NetGuard-GNN is an insider threat detection framework that integrates graph-based deep learning and statistical modeling.  \n",
    "In this notebook, pre-collected or mock datasets are used to emulate the full system workflow, where operational data is originally obtained from hardware-level monitoring modules.\n",
    "\n",
    "**Workflow Overview:**\n",
    "1. Load heterogeneous graph and tabular datasets.\n",
    "2. Train a Graph Neural Network (GNN) to capture structural patterns in user–resource interactions.\n",
    "3. Train a One-Class SVM (OCSVM) on tabular features to detect statistical anomalies.\n",
    "4. Fuse model outputs to generate ranked lists of high-risk users and resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b2f6c",
   "metadata": {},
   "source": [
    "## Mock Data Generation\n",
    "\n",
    "### Data Generation for GNN\n",
    "\n",
    "This step generates a synthetic activity log when no hardware-captured data is available:\n",
    "\n",
    "- **Purpose:** Provide test input for the pipeline during development.  \n",
    "- **Contents:**  \n",
    "  - Timestamps (hourly intervals)  \n",
    "  - User IDs and Resource IDs  \n",
    "  - Actions (`login`, `read`, `download`)  \n",
    "  - Data size in bytes  \n",
    "  - Operation success flag (0 or 1)  \n",
    "- **Output:** Saved as `verilog_output.csv` in the `data` directory for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "CSV_FILE = os.path.join(DATA_DIR, \"verilog_output.csv\")\n",
    "\n",
    "def mock_logs(path):\n",
    "    \"\"\"Generate a small mock CSV if no Verilog output exists yet.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"timestamp\": pd.date_range(\"2025-01-01\", periods=20, freq=\"H\"),\n",
    "        \"user_id\": [\"U001\",\"U002\",\"U003\",\"U001\",\"U002\"] * 4,\n",
    "        \"resource_id\": [\"R01\",\"R02\",\"R03\",\"R01\",\"R03\"] * 4,\n",
    "        \"action\": np.random.choice([\"login\",\"read\",\"download\"], size=20),\n",
    "        \"bytes\": np.random.randint(500, 5000, size=20),\n",
    "        \"success\": np.random.choice([0, 1], size=20, p=[0.1, 0.9])\n",
    "    })\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[MOCK] Created mock log file at {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f793e01",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "This step ensures the required activity log is available and loads it for processing:\n",
    "\n",
    "- **Check & Create:**  \n",
    "  - If `verilog_output.csv` does not exist, create the `data` directory (if missing) and generate mock logs.  \n",
    "- **Load Data:**  \n",
    "  - Read the CSV file into a DataFrame.  \n",
    "  - Parse the `timestamp` column as datetime objects.  \n",
    "- **Info Output:**  \n",
    "  - Display the total number of loaded events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1eae823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 20 events from data\\verilog_output.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CSV_FILE):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    mock_logs(CSV_FILE)\n",
    "\n",
    "df = pd.read_csv(CSV_FILE, parse_dates=[\"timestamp\"])\n",
    "print(f\"[INFO] Loaded {len(df)} events from {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38695ea",
   "metadata": {},
   "source": [
    "### Graph Construction\n",
    "\n",
    "This step transforms the event log into a heterogeneous graph structure for GNN processing:\n",
    "\n",
    "- **Node Index Mapping:**  \n",
    "  - Assign unique indices to each user (`user_id`) and resource (`resource_id`).  \n",
    "\n",
    "- **Edge Creation:**  \n",
    "  - Construct `edge_index` tensors representing user → resource access events.  \n",
    "  - Attach edge attributes:  \n",
    "    - Data size in bytes  \n",
    "    - Operation success flag  \n",
    "    - Encoded action type  \n",
    "\n",
    "- **Node Feature Extraction:**  \n",
    "  - **Users:** Mean bytes transferred and mean success rate.  \n",
    "  - **Resources:** Mean bytes transferred.  \n",
    "\n",
    "- **HeteroData Assembly:**  \n",
    "  - Create `user` and `resource` node types with corresponding features.  \n",
    "  - Add `accessed` edges and attributes between users and resources.  \n",
    "\n",
    "- **Save Graph:**  \n",
    "  - Store as `graph_data.pt` for downstream GNN training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e0de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved GNN graph data to data\\graph_data.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarva\\AppData\\Local\\Temp\\ipykernel_18576\\3778473747.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  edge_index = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "user_ids = df[\"user_id\"].unique()\n",
    "user_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "\n",
    "# Map resource_id -> resource node index\n",
    "resource_ids = df[\"resource_id\"].unique()\n",
    "res_map = {rid: i for i, rid in enumerate(resource_ids)}\n",
    "\n",
    "# Build edge index (user -> resource)\n",
    "src_nodes = df[\"user_id\"].map(user_map).to_numpy()\n",
    "dst_nodes = df[\"resource_id\"].map(res_map).to_numpy()\n",
    "edge_index = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)\n",
    "\n",
    "# Edge attributes (bytes, success, action_code)\n",
    "edge_attr = torch.tensor(\n",
    "    np.stack([\n",
    "        df[\"bytes\"].to_numpy(),\n",
    "        df[\"success\"].to_numpy(),\n",
    "        pd.Categorical(df[\"action\"]).codes\n",
    "    ], axis=1),\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "# Node features (simple averages)\n",
    "user_feats = np.zeros((len(user_ids), 2))\n",
    "res_feats = np.zeros((len(resource_ids), 1))\n",
    "\n",
    "for uid, idx in user_map.items():\n",
    "    subset = df[df[\"user_id\"] == uid]\n",
    "    user_feats[idx, 0] = subset[\"bytes\"].mean()\n",
    "    user_feats[idx, 1] = subset[\"success\"].mean()\n",
    "\n",
    "for rid, idx in res_map.items():\n",
    "    subset = df[df[\"resource_id\"] == rid]\n",
    "    res_feats[idx, 0] = subset[\"bytes\"].mean()\n",
    "\n",
    "user_x = torch.tensor(user_feats, dtype=torch.float)\n",
    "res_x = torch.tensor(res_feats, dtype=torch.float)\n",
    "\n",
    "# Create HeteroData object for GNN\n",
    "data = HeteroData()\n",
    "data[\"user\"].x = user_x\n",
    "data[\"resource\"].x = res_x\n",
    "data[\"user\", \"accessed\", \"resource\"].edge_index = edge_index\n",
    "data[\"user\", \"accessed\", \"resource\"].edge_attr = edge_attr\n",
    "\n",
    "# Save graph data\n",
    "torch.save(data, os.path.join(DATA_DIR, \"graph_data.pt\"))\n",
    "print(f\"[INFO] Saved GNN graph data to {os.path.join(DATA_DIR, 'graph_data.pt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241c7c1",
   "metadata": {},
   "source": [
    "**Note:** The GNN graph data has been successfully saved and is available at `data\\graph_data.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce117643",
   "metadata": {},
   "source": [
    "### Tabular Feature Generation\n",
    "\n",
    "This step prepares event-level features for the One-Class SVM (OC-SVM) anomaly detection model:\n",
    "\n",
    "- **Feature Extraction:**  \n",
    "  - `bytes` – Size of data transferred.  \n",
    "  - `success` – Operation success flag (0 or 1).  \n",
    "  - `action_code` – Encoded action type (`login`, `read`, `download`).  \n",
    "  - `hour` – Hour of the event timestamp.  \n",
    "  - `dayofweek` – Day of the week of the event.  \n",
    "\n",
    "- **Output:**  \n",
    "  - Save features to `tabular_features.csv` in the `data` directory for OC-SVM training.  \n",
    "\n",
    "- **Progress Update:**  \n",
    "  - Indicate completion of Step 1 (data preparation) and readiness for Step 2 (model training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11af5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved OC-SVM features to data\\tabular_features.csv\n",
      "[DONE] Step 1 complete — ready for Step 2 training\n"
     ]
    }
   ],
   "source": [
    "tabular_df = pd.DataFrame({\n",
    "    \"bytes\": df[\"bytes\"],\n",
    "    \"success\": df[\"success\"],\n",
    "    \"action_code\": pd.Categorical(df[\"action\"]).codes,\n",
    "    \"hour\": df[\"timestamp\"].dt.hour,\n",
    "    \"dayofweek\": df[\"timestamp\"].dt.dayofweek\n",
    "})\n",
    "tabular_df.to_csv(os.path.join(DATA_DIR, \"tabular_features.csv\"), index=False)\n",
    "print(f\"[INFO] Saved OC-SVM features to {os.path.join(DATA_DIR, 'tabular_features.csv')}\")\n",
    "\n",
    "print(\"[DONE] Step 1 complete — ready for Step 2 training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d8eaf",
   "metadata": {},
   "source": [
    "**Note:** The OC-SVM features have been successfully saved and Step 1 is complete. Features are available at `data\\tabular_features.csv`. Ready to proceed with Step 2 training.\n",
    "\n",
    "---\n",
    "\n",
    "## Training GNN and One-Class SVM(OC-SVM) Models\n",
    "\n",
    "### Data File Paths Initialization\n",
    "\n",
    "This code snippet sets up the directory and file paths for storing graph data, tabular features, and Verilog output in the `data` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9246b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "GRAPH_FILE = os.path.join(DATA_DIR, \"graph_data.pt\")\n",
    "TAB_FILE = os.path.join(DATA_DIR, \"tabular_features.csv\")\n",
    "CSV_FILE = os.path.join(DATA_DIR, \"verilog_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0022f",
   "metadata": {},
   "source": [
    "### Min-Max Scaling of Scores\n",
    "\n",
    "- **Function:** `scale_scores`\n",
    "- **Purpose:** Normalize input data to the range `[0, 1]` for consistent feature scaling.\n",
    "- **Input:** `x` — NumPy array (`np.ndarray`) containing the values to be scaled.\n",
    "- **Methodology:** \n",
    "  - Reshape the input array to 2D to ensure compatibility with `MinMaxScaler`.\n",
    "  - Fit the `MinMaxScaler` to the data.\n",
    "  - Transform the data and flatten it back to 1D.\n",
    "- **Output:** Scaled NumPy array (`np.ndarray`) with values constrained between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7471ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_scores(x: np.ndarray) -> np.ndarray:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_2d = x.reshape(-1, 1)\n",
    "    scaler.fit(x_2d)\n",
    "    return scaler.transform(x_2d).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4f60",
   "metadata": {},
   "source": [
    "### Load Heterogeneous Graph Data\n",
    "\n",
    "- **Function:** `load_graph`  \n",
    "- **Purpose:** Load graph data from disk and move it to GPU if available.  \n",
    "- **Dependencies:** `torch`, `torch_geometric.data.HeteroData`, `BaseStorage`, `NodeStorage`, `EdgeStorage`  \n",
    "- **Process:**  \n",
    "  - Register safe globals for PyTorch serialization  \n",
    "  - Load `HeteroData` from `GRAPH_FILE`  \n",
    "  - Transfer data to CUDA if available, otherwise CPU  \n",
    "- **Output:** `HeteroData` object ready for GNN processing  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f899ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph():\n",
    "    import torch\n",
    "    from torch_geometric.data import HeteroData\n",
    "    from torch_geometric.data.storage import BaseStorage, NodeStorage, EdgeStorage\n",
    "\n",
    "\n",
    "    torch.serialization.add_safe_globals([BaseStorage, NodeStorage, EdgeStorage])\n",
    "    data: HeteroData = torch.load(GRAPH_FILE)\n",
    "    return data.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "data = load_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041ea96",
   "metadata": {},
   "source": [
    "### Edge Splitting Function for Graph Data\n",
    "\n",
    "This code defines a function `split_edges` that splits the edges of a graph into training and validation sets. It uses **scikit-learn's `train_test_split`** to randomly partition the edges based on a specified training ratio. The function:\n",
    "\n",
    "- Converts the edge index from a PyTorch tensor to a NumPy array for splitting.\n",
    "- Splits edges into training and validation sets according to the `train_ratio`.\n",
    "- Converts the resulting splits back to PyTorch tensors on the same device as the input.\n",
    "- Returns the training and validation edge indices.  \n",
    "\n",
    "The last line demonstrates splitting the `'user' → 'resource'` edges from the heterogeneous graph data into training edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65bd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges(edge_index, train_ratio=0.85):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    \n",
    "    edges = edge_index.cpu().numpy().T  # shape: (num_edges, 2)\n",
    "    train_edges, val_edges = train_test_split(edges,\n",
    "                                            train_size=train_ratio,\n",
    "                                            shuffle=True,\n",
    "                                            random_state=42)\n",
    "    \n",
    "    train_edges = torch.tensor(train_edges.T, dtype=torch.long, device=edge_index.device)\n",
    "    val_edges = torch.tensor(val_edges.T, dtype=torch.long, device=edge_index.device)\n",
    "    \n",
    "    return train_edges, val_edges\n",
    "\n",
    "train_pos, _ = split_edges(data[('user', 'accessed', 'resource')].edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca43b4b",
   "metadata": {},
   "source": [
    "### Heterogeneous Graph Neural Network (HeteroGNN) Definition\n",
    "\n",
    "This code defines a **Heterogeneous Graph Neural Network (HeteroGNN)** using PyTorch Geometric. Key components:\n",
    "\n",
    "- **HeteroGNN class**:\n",
    "  - Inherits from `torch.nn.Module`.\n",
    "  - Uses `HeteroConv` to handle multiple types of edges in a heterogeneous graph.\n",
    "  - Applies `SAGEConv` on:\n",
    "    - `('user', 'accessed', 'resource')` edges\n",
    "    - `('resource', 'rev_accessed', 'user')` edges\n",
    "  - Aggregates messages using the `'mean'` method.\n",
    "  - Applies ReLU activation to node embeddings after convolution.\n",
    "\n",
    "- **edge_score function**:\n",
    "  - Computes a similarity score between user and resource embeddings using a dot product.\n",
    "  - Useful for predicting edge existence or link strength in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601954bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class HeteroGNN(nn.Module):\n",
    "    def __init__(self, hidden=64):\n",
    "        super().__init__()\n",
    "        self.convs = HeteroConv({\n",
    "            ('user', 'accessed', 'resource'): SAGEConv((-1, -1), hidden),\n",
    "            ('resource', 'rev_accessed', 'user'): SAGEConv((-1, -1), hidden)\n",
    "        }, aggr='mean')\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.convs(x_dict, edge_index_dict)\n",
    "        x_dict = {k: v.relu() for k, v in x_dict.items()}\n",
    "        return x_dict\n",
    "\n",
    "def edge_score(u_emb, r_emb):\n",
    "    return (u_emb * r_emb).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e11bc",
   "metadata": {},
   "source": [
    "### Training HeteroGNN on Heterogeneous Graph Data\n",
    "\n",
    "This code defines the `train_gnn` function to train the previously defined **HeteroGNN** on a heterogeneous graph dataset. Key steps:\n",
    "\n",
    "- **Model and Loss Initialization**:\n",
    "  - Instantiate `HeteroGNN` with 64 hidden units.\n",
    "  - Use `BCEWithLogitsLoss` for edge prediction.\n",
    "  - Optimizer: Adam with learning rate `1e-3` and weight decay `1e-4`.\n",
    "\n",
    "- **Training Loop** (`epochs=50` by default):\n",
    "  - Forward pass: Compute node embeddings for `'user'` and `'resource'`.\n",
    "  - Positive edges: Compute logits for existing edges using `edge_score`.\n",
    "  - Negative edges: Sample non-existent edges using `negative_sampling` and compute their logits.\n",
    "  - Concatenate positive and negative logits; compute BCE loss and update model parameters.\n",
    "\n",
    "- **Saving Model Weights**:\n",
    "  - Extract trained parameters and save them using `pickle` to `gnn_model_manual.pth`.\n",
    "\n",
    "This function trains the GNN to distinguish existing edges from randomly sampled non-existent edges in the heterogeneous graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803719fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import os,pickle\n",
    "\n",
    "def train_gnn(data, train_pos, epochs=50):\n",
    "    device = data['user'].x.device\n",
    "    gnn = HeteroGNN(hidden=64).to(device)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    opt = torch.optim.Adam(gnn.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        gnn.train()\n",
    "        opt.zero_grad()\n",
    "        out = gnn(\n",
    "            {'user': data['user'].x, 'resource': data['resource'].x},\n",
    "            {\n",
    "                ('user', 'accessed', 'resource'): train_pos,\n",
    "                ('resource', 'rev_accessed', 'user'): train_pos.flip(0)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        user_z, res_z = out['user'], out['resource']\n",
    "        u_pos, r_pos = train_pos[0], train_pos[1]\n",
    "        pos_logits = edge_score(user_z[u_pos], res_z[r_pos])\n",
    "\n",
    "        neg_edges = negative_sampling(\n",
    "            edge_index=train_pos,\n",
    "            num_nodes=(data['user'].x.size(0), data['resource'].x.size(0)),\n",
    "            num_neg_samples=u_pos.size(0),\n",
    "            method='sparse'\n",
    "        )\n",
    "        u_neg, r_neg = neg_edges[0], neg_edges[1]\n",
    "        neg_logits = edge_score(user_z[u_neg], res_z[r_neg])\n",
    "\n",
    "        logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
    "        labels = torch.cat([torch.ones_like(pos_logits), torch.zeros_like(neg_logits)], dim=0)\n",
    "        loss = bce(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return gnn\n",
    "\n",
    "gnn = train_gnn(data, train_pos)\n",
    "weights = {name: param.detach().cpu() for name, param in gnn.named_parameters()}\n",
    "\n",
    "path = os.path.join(DATA_DIR, \"gnn_model_manual.pth\")\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(weights, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d114c7",
   "metadata": {},
   "source": [
    "### Training One-Class SVM (OC-SVM) on Tabular Features\n",
    "\n",
    "This code defines the `train_ocsvm` function to train a **One-Class SVM** for anomaly detection on tabular features. Key steps:\n",
    "\n",
    "- **Data Loading**:\n",
    "  - Load tabular features from `TAB_FILE` into a DataFrame.\n",
    "  - Use all columns as features for training.\n",
    "\n",
    "- **Pipeline Setup**:\n",
    "  - **StandardScaler**: Standardizes features to zero mean and unit variance.\n",
    "  - **OneClassSVM**: RBF kernel, `gamma='scale'`, and `nu=0.05` to detect anomalies.\n",
    "\n",
    "- **Model Training and Saving**:\n",
    "  - Fit the pipeline on the tabular features.\n",
    "  - Save the trained pipeline using `joblib` to `ocsvm.joblib`.\n",
    "\n",
    "- **Return Value**:\n",
    "  - Returns the trained OC-SVM pipeline for later use in anomaly detection tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd84806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ocsvm():\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from joblib import dump\n",
    "\n",
    "    tab = pd.read_csv(TAB_FILE)\n",
    "    feat_cols = tab.columns.tolist()\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ocsvm\", OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05))\n",
    "    ])\n",
    "    pipe.fit(tab[feat_cols])\n",
    "    \n",
    "    dump(pipe, os.path.join(DATA_DIR, \"ocsvm.joblib\"))\n",
    "    return pipe\n",
    "\n",
    "ocsvm_model = train_ocsvm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ab9b1",
   "metadata": {},
   "source": [
    "## Anomaly Score Computation\n",
    "\n",
    "### Computing GNN-Based Anomaly Scores\n",
    "\n",
    "This code defines the `get_gnn_anomaly_scores` function to compute anomaly scores for nodes in a heterogeneous graph using a trained **HeteroGNN**. Key steps:\n",
    "\n",
    "- **Input and Setup**:\n",
    "  - Receives a trained `gnn` model and `data` containing nodes and edges.\n",
    "  - Ensures node types `'user'` and `'resource'` exist in the data.\n",
    "  - Prepares the edge index dictionary for message passing.\n",
    "\n",
    "- **Forward Pass**:\n",
    "  - Computes node embeddings using the GNN in evaluation mode (`torch.no_grad()`).\n",
    "  - Checks that embeddings for both `'user'` and `'resource'` nodes are returned.\n",
    "\n",
    "- **Anomaly Scoring**:\n",
    "  - Computes edge probabilities using the `edge_score` and a sigmoid function.\n",
    "  - Calculates a \"surprisal\" score (`1 - probability`) for each edge.\n",
    "  - Aggregates scores per node and normalizes by the number of connected edges.\n",
    "\n",
    "- **Output**:\n",
    "  - Returns two arrays: `u_scores` for users and `r_scores` for resources, representing node-level anomaly scores.\n",
    "  - Example usage: `u_gnn, r_gnn = get_gnn_anomaly_scores(gnn, data)` and normalized scores with `u_gnn_norm = scale_scores(u_gnn)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f05d439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types in data: ['user', 'resource']\n",
      "Edge types in data: [('user', 'accessed', 'resource')]\n"
     ]
    }
   ],
   "source": [
    "def get_gnn_anomaly_scores(gnn, data):\n",
    "    import torch\n",
    "\n",
    "    device = next(gnn.parameters()).device\n",
    "    gnn.eval()\n",
    "\n",
    "    print(\"Node types in data:\", data.node_types)\n",
    "    print(\"Edge types in data:\", data.edge_types)\n",
    "\n",
    "    try:\n",
    "        x_dict = {\n",
    "            'user': data['user'].x.to(device),\n",
    "            'resource': data['resource'].x.to(device)\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Missing expected node type in data: {e}\")\n",
    "\n",
    "    edge_index_dict = {}\n",
    "    if ('user', 'accessed', 'resource') in data.edge_types:\n",
    "        eidx = data[('user', 'accessed', 'resource')].edge_index.to(device)\n",
    "        edge_index_dict[('user', 'accessed', 'resource')] = eidx\n",
    "        edge_index_dict[('resource', 'rev_accessed', 'user')] = eidx.flip(0)\n",
    "    else:\n",
    "        raise ValueError(\"Expected edge type ('user','accessed','resource') not found in data.\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = gnn(x_dict, edge_index_dict)\n",
    "\n",
    "    if not isinstance(emb, dict):\n",
    "        raise ValueError(f\"GNN output is not a dict. Got type: {type(emb)}\")\n",
    "\n",
    "    user_z = emb.get('user', None)\n",
    "    res_z = emb.get('resource', None)\n",
    "    if user_z is None or res_z is None:\n",
    "        raise ValueError(\n",
    "            f\"GNN output missing 'user' or 'resource' embeddings. Got keys: {list(emb.keys())}\"\n",
    "        )\n",
    "\n",
    "    u_all = edge_index_dict[('user', 'accessed', 'resource')][0]\n",
    "    r_all = edge_index_dict[('user', 'accessed', 'resource')][1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lp = torch.sigmoid(edge_score(user_z[u_all], res_z[r_all]))\n",
    "\n",
    "    u_scores = torch.zeros(user_z.size(0), device=device)\n",
    "    u_count = torch.zeros_like(u_scores)\n",
    "    r_scores = torch.zeros(res_z.size(0), device=device)\n",
    "    r_count = torch.zeros_like(r_scores)\n",
    "\n",
    "    for i in range(u_all.size(0)):\n",
    "        u, r = u_all[i], r_all[i]\n",
    "        surprisal = 1.0 - lp[i]\n",
    "        u_scores[u] += surprisal\n",
    "        u_count[u] += 1\n",
    "        r_scores[r] += surprisal\n",
    "        r_count[r] += 1\n",
    "\n",
    "    u_scores = (u_scores / torch.clamp(u_count, min=1)).cpu().numpy()\n",
    "    r_scores = (r_scores / torch.clamp(r_count, min=1)).cpu().numpy()\n",
    "\n",
    "    return u_scores, r_scores\n",
    "\n",
    "u_gnn, r_gnn = get_gnn_anomaly_scores(gnn, data)\n",
    "u_gnn_norm = scale_scores(u_gnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a130a",
   "metadata": {},
   "source": [
    "- **Node types in data:** Lists all node categories present in the heterogeneous graph. Here, the graph contains `'user'` and `'resource'` nodes.  \n",
    "- **Edge types in data:** Lists all edge relationships between node types. Here, the graph has a single edge type: `'user' → 'resource'` via `'accessed'`.  \n",
    "\n",
    "This confirms that the input data matches the expected structure for the GNN anomaly scoring function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd1687",
   "metadata": {},
   "source": [
    "### Computing OC-SVM-Based Anomaly Scores\n",
    "\n",
    "This code defines the `get_ocsvm_anomaly_scores` function to calculate anomaly scores using a trained **One-Class SVM (OC-SVM)** pipeline. Key steps:\n",
    "\n",
    "- **Data Loading**:\n",
    "  - Reads the original raw CSV (`CSV_FILE`) and the tabular features (`TAB_FILE`).\n",
    "\n",
    "- **Anomaly Scoring**:\n",
    "  - Applies the OC-SVM `decision_function` to the tabular features.\n",
    "  - Converts the decision scores into anomaly scores and normalizes them using `scale_scores`.\n",
    "\n",
    "- **Aggregation**:\n",
    "  - Assigns anomaly scores to individual events in the raw data.\n",
    "  - Computes per-user anomaly scores by averaging event-level anomalies for each `user_id`.\n",
    "  - Normalizes the user-level scores and handles missing users by filling with 0.0.\n",
    "\n",
    "- **Return Value**:\n",
    "  - `ocsvm_user`: Normalized anomaly scores for each user.\n",
    "  - `user_ids`: List of all user IDs corresponding to the scores.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  ocsvm_model = train_ocsvm()\n",
    "  ocsvm_user, user_ids = get_ocsvm_anomaly_scores(ocsvm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48903bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocsvm_anomaly_scores(pipe):\n",
    "    raw = pd.read_csv(CSV_FILE, parse_dates=[\"timestamp\"])\n",
    "    tab = pd.read_csv(TAB_FILE)\n",
    "    \n",
    "    dfn = pipe.decision_function(tab)\n",
    "    ocsvm_event_anom = scale_scores(-dfn)\n",
    "    \n",
    "    raw[\"ocsvm_event_anom\"] = ocsvm_event_anom\n",
    "    \n",
    "    ocsvm_user = raw.groupby(\"user_id\")[\"ocsvm_event_anom\"].mean().reindex(\n",
    "        raw[\"user_id\"].astype(\"category\").cat.categories\n",
    "    ).fillna(0.0).values\n",
    "    \n",
    "    ocsvm_user = scale_scores(ocsvm_user)\n",
    "    user_ids = raw[\"user_id\"].astype(\"category\").cat.categories.tolist()\n",
    "    \n",
    "    return ocsvm_user, user_ids\n",
    "\n",
    "ocsvm_model = train_ocsvm()\n",
    "ocsvm_user, user_ids = get_ocsvm_anomaly_scores(ocsvm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e0261",
   "metadata": {},
   "source": [
    "### Saving and Aggregating Anomaly Scores\n",
    "\n",
    "This code defines the `save_results` function to combine, rank, and save anomaly scores for users and resources. Key steps:\n",
    "\n",
    "- **Ensemble Scoring**:\n",
    "  - Combines normalized GNN (`u_gnn_norm`) and OC-SVM (`ocsvm_user`) user scores using a simple average (`0.5 * u_gnn_norm + 0.5 * ocsvm_user`).\n",
    "\n",
    "- **DataFrames Creation**:\n",
    "  - **User Scores**: Includes individual GNN and OC-SVM scores, the ensemble score, and is sorted in descending order of anomaly.\n",
    "  - **Resource Scores**: Includes GNN-based resource scores (normalized) and sorted by descending anomaly.\n",
    "\n",
    "- **Saving Results**:\n",
    "  - Saves user scores to `user_scores.csv`.\n",
    "  - Saves resource scores to `resource_scores.csv`.\n",
    "\n",
    "- **Return Value**:\n",
    "  - Returns the sorted user and resource score DataFrames.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  resource_ids = pd.read_csv(CSV_FILE)[\"resource_id\"].astype(\"category\").cat.categories.tolist()\n",
    "  user_scores, _ = save_results(u_gnn_norm, ocsvm_user, user_ids, r_gnn, resource_ids)\n",
    "  print(user_scores.head(5))  # Display top suspicious users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "881c1db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top suspicious users:\n",
      "  user_id  gnn_score  ocsvm_score  ensemble_score\n",
      "2    U003        0.0     1.000000        0.500000\n",
      "0    U001        0.0     0.188167        0.094083\n",
      "1    U002        0.0     0.000000        0.000000\n"
     ]
    }
   ],
   "source": [
    "def save_results(u_gnn_norm, ocsvm_user, user_ids, r_gnn, resource_ids):\n",
    "    ensemble = 0.5 * u_gnn_norm + 0.5 * ocsvm_user\n",
    "    user_scores = pd.DataFrame({\n",
    "        \"user_id\": user_ids,\n",
    "        \"gnn_score\": u_gnn_norm,\n",
    "        \"ocsvm_score\": ocsvm_user,\n",
    "        \"ensemble_score\": ensemble\n",
    "    }).sort_values(\"ensemble_score\", ascending=False)\n",
    "    resource_scores = pd.DataFrame({\n",
    "        \"resource_id\": resource_ids,\n",
    "        \"gnn_score\": scale_scores(r_gnn)\n",
    "    }).sort_values(\"gnn_score\", ascending=False)\n",
    "    user_scores.to_csv(os.path.join(DATA_DIR, \"user_scores.csv\"), index=False)\n",
    "    resource_scores.to_csv(os.path.join(DATA_DIR, \"resource_scores.csv\"), index=False)\n",
    "    return user_scores, resource_scores\n",
    "\n",
    "resource_ids = pd.read_csv(CSV_FILE)[\"resource_id\"].astype(\"category\").cat.categories.tolist()\n",
    "user_scores, _ = save_results(u_gnn_norm, ocsvm_user, user_ids, r_gnn, resource_ids)\n",
    "print(\"\\nTop suspicious users:\")\n",
    "print(user_scores.head(min(5, len(user_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653cf1d8",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "\n",
    "The table shows the top suspicious users ranked by their ensemble anomaly scores. Higher scores indicate higher anomaly likelihood. The columns represent:\n",
    "\n",
    "- **gnn_score**: Node-level anomaly score predicted by the GNN.  \n",
    "- **ocsvm_score**: Anomaly score predicted by the OC-SVM.  \n",
    "- **ensemble_score**: Combined score (average of GNN and OC-SVM) used for ranking users.\n",
    "\n",
    "In this example, **U003** is flagged as the most suspicious user.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
